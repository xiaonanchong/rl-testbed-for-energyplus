{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000000/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=0, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000000 -w log/output/episode-00000000/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000000/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_44_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_44_act\n",
      "[38.582504  23.155764   5.8091307  3.6109972]\n",
      "[6.82500000e+00 2.30002710e+01 2.30002740e+01 4.60615714e+04\n",
      " 4.14012577e+04 4.66031370e+03]\n",
      "1.3046185227572893\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('gym_energyplus:EnergyPlus-v0')\n",
    "observation = env.reset()\n",
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "print(action)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in /home/xiaonanchong/venv/rl/lib/python3.5/site-packages (0.1.3)\n",
      "0 ---activation: sigmoid\n",
      "1 ---activation: sigmoid\n",
      "0 ---activation: sigmoid\n",
      "1 ---activation: sigmoid\n",
      "Starting new environment\n",
      "start_instance(): idx=0, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000000 -w log/output/episode-00000000/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000000/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000000/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=1, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000001 -w log/output/episode-00000001/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000001/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "0 :loss= 14.09194062152935\n",
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000001/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=2, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000002 -w log/output/episode-00000002/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000002/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000002/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=3, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000003 -w log/output/episode-00000003/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000003/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "1 :loss= 14.09194062152935\n",
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000003/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=4, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000004 -w log/output/episode-00000004/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000004/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "EnergyPlusEnv: Severe error(s) occurred. Error count: -1\n",
      "EnergyPlusEnv: Check contents of log/output/episode-00000004/eplusout.err\n",
      "Starting new environment\n",
      "start_instance(): idx=5, model_file=/home/xiaonanchong/rl-testbed-for-energyplus/EnergyPlus/Model/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "start_instance(): weather_files[0]=/usr/local/EnergyPlus-8-8-0/WeatherData/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw\n",
      "Starting EnergyPlus with command: /usr/local/EnergyPlus-8-8-0/energyplus -r -x -d log/output/episode-00000005 -w log/output/episode-00000005/USA_CA_San.Francisco.Intl.AP.724940_TMY3.epw log/output/episode-00000005/2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.idf\n",
      "PipeIo.readline: Opening OBS pipe [/tmp/extctrl_152_obs]\n",
      "PipeIo.writeline: Opened ACT pipe /tmp/extctrl_152_act\n",
      "2 :loss= 14.09194062152935\n"
     ]
    }
   ],
   "source": [
    "!pip install import_ipynb\n",
    "import import_ipynb\n",
    "import nn_lib\n",
    "from nn_lib import * #MultiLayerNetwork, save_network, load_network\n",
    "\n",
    "import gym\n",
    "env = gym.make('gym_energyplus:EnergyPlus-v0')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#S + A -> R\n",
    "class critic(MultiLayerNetwork):\n",
    "    def __init__(self, env):\n",
    "        action_dim = len(env.action_space.low)\n",
    "        observation_dim = len(env.observation_space.low)\n",
    "        \n",
    "        neurons = [8, 1]\n",
    "        activations = [\"sigmoid\", \"sigmoid\"]\n",
    "        MultiLayerNetwork.__init__(self, action_dim+observation_dim, neurons, activations)\n",
    "    '''  \n",
    "    def forward(self, sa):\n",
    "        action_dim = len(env.action_space.low)\n",
    "        observation_dim = len(env.observation_space.low)\n",
    "        x = sa.reshape((1, action_dim+observation_dim))\n",
    "        y = MultiLayerNetwork.forward(self, x)\n",
    "        return y[0]\n",
    "    '''\n",
    "#S -> a\n",
    "class actor(MultiLayerNetwork):\n",
    "    def __init__(self, env):\n",
    "        action_dim = len(env.action_space.low)\n",
    "        observation_dim = len(env.observation_space.low)\n",
    "        \n",
    "        neurons = [8, action_dim]\n",
    "        activations = [\"sigmoid\", \"sigmoid\"]\n",
    "        MultiLayerNetwork.__init__(self, observation_dim, neurons, activations) \n",
    "        \n",
    "    def forward(self, s1):\n",
    "        observation_dim = len(env.observation_space.low)\n",
    "        x = s1.reshape((1, observation_dim))\n",
    "        y = MultiLayerNetwork.forward(self, x)\n",
    "        return y[0]*2 -1\n",
    "\n",
    "def ddpg(env):\n",
    "    action_dim = len(env.action_space.low)\n",
    "    observation_dim = len(env.observation_space.low)\n",
    "    M = 3 # episodes\n",
    "    T = 10 # timestep\n",
    "    N = 3 # mini batch size\n",
    "    gamma = 0.9 # discounted factor\n",
    "    tao = 0.5\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    #random initialize critic network and actor network\n",
    "    net_critic = critic(env)\n",
    "    net_actor = actor(env)\n",
    "    \n",
    "    #initialize target network \n",
    "    target_Q = net_critic\n",
    "    target_mu = net_actor\n",
    "    \n",
    "    #initialize replay buffer \n",
    "    R = []\n",
    "    \n",
    "    for episode in range(M):\n",
    "        s1 = env.reset()\n",
    "        for t in range(T):\n",
    "            a1 = net_actor(s1) + np.random.normal(0, 1.0/(episode+1),1)\n",
    "            s2, r1, done, info = env.step(a1)\n",
    "            R.append([s1, a1, r1, s2])\n",
    "            s1 = s2\n",
    "            \n",
    "            if len(R)>N:\n",
    "                #sample a random mini-batch of N transitions from R\n",
    "                minibatch = random.sample(R, N)\n",
    "                \n",
    "                #set y_i\n",
    "                delta = np.zeros(action_dim)\n",
    "                y = []\n",
    "                #x = np.zeros(N,action_dim+observation_dim)\n",
    "                x = []\n",
    "                for i in range(N):\n",
    "                    [s1, a1, r1, s2] = minibatch[i]\n",
    "                    a2 = target_mu(s2)\n",
    "                    y.append(r1 + gamma*target_Q(np.append(s2, a2))[0])\n",
    "                    #x[i] = np.append(s1, a1)\n",
    "                    x.append(np.append(s1, a1))\n",
    "                    \n",
    "                    a_1 = net_actor(s1)\n",
    "                    o = net_critic(np.append(s1, a_1))[0]\n",
    "                    delta += net_critic.backward([o])[0][-action_dim:]\n",
    "                    net_critic.update_params(learning_rate)\n",
    "                    \n",
    "                delta = delta/N\n",
    "                print(delta)\n",
    "                \n",
    "                #update critic\n",
    "                trainer = Trainer(\n",
    "                    network=net_critic,\n",
    "                    batch_size=N,\n",
    "                    nb_epoch=1,\n",
    "                    learning_rate=0.01,\n",
    "                    loss_fun=\"mse\",\n",
    "                    shuffle_flag=True,\n",
    "                    )\n",
    "                #print('-----')\n",
    "                #print(np.asarray(x))\n",
    "                #print(np.asarray(y))\n",
    "                trainer.train(np.asarray(x), np.asarray(y))\n",
    "                \n",
    "                #update actor policy using the sampled policy gradient\n",
    "                net_actor.backward(delta)\n",
    "                net_actor.update_params(learning_rate)\n",
    "                \n",
    "                #update the target networks\n",
    "                target_Q = con(net_critic, target_Q, tao)\n",
    "                target_mu = con(net_actor, target_mu, tao)\n",
    "                \n",
    "                \n",
    "        ## evaluation\n",
    "        s = env.reset()\n",
    "        loss = 0\n",
    "        for t in range(20):\n",
    "            a = net_actor(s)\n",
    "            s, r, _, _ = env.step(a)\n",
    "            loss+=r\n",
    "        print(episode,':loss=',loss)\n",
    "                \n",
    "def con(net1, net2, tao):\n",
    "    for i in range(len(net1._layers)):\n",
    "        if i%2 == 0:\n",
    "            net1._layers[i]._W = tao*net1._layers[i]._W + (1-tao)*net2._layers[i]._W\n",
    "            net1._layers[i]._b = tao*net1._layers[i]._b + (1-tao)*net2._layers[i]._b\n",
    "    return net1\n",
    "    \n",
    "ddpg(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
